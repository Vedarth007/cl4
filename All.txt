.
































.





















































.











1.Problem Statement – Real estate agents want help to predict the house price for regions in the USA. He gave you the dataset to work on and you decided to use the Linear Regression Model. Create a model that will help him to estimate what the house would sell for. URL for a dataset: https://github.com/huzaifsayed/Linear-Regression-Model-for-House-Price Prediction/blob/master/USA_Housing.csv

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error

# 1. Load the dataset
#url = "https://raw.githubusercontent.com/huzaifsayed/Linear-Regression-Model-for-House-PricePrediction/master/USA_Housing.csv"
df = pd.read_csv('USA_Housing.csv')

# 2. Select features & target
X = df[[
    "Avg. Area Income",
    "Avg. Area House Age",
    "Avg. Area Number of Rooms",
    "Area Population"
]]
y = df["Price"]

# 3. Split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=0
)

# 4. Train
model = LinearRegression()
model.fit(X_train, y_train)

# 5. Evaluate
preds = model.predict(X_test)
rmse = mean_squared_error(y_test, preds, squared=False)
print(f"RMSE: {rmse:.2f}")

#or

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

# 1. Load data
#url = "https://raw.githubusercontent.com/huzaifsayed/Linear-Regression-Model-for-House-PricePrediction/master/USA_Housing.csv"
df = pd.read_csv('USA_Housing.csv')

# 2. Create binary target: 1 if price above median, else 0
df['HighValue'] = (df['Price'] > df['Price'].median()).astype(int)

# 3. Select features & target
X = df[[
    "Avg. Area Inciome",
    "Avg. Area House Age",
    "Avg. Area Number of Rooms",
    "Area Population"
]]
y = df['HighValue']

# 4. Split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=0
)

# 5. Train classifier
model = LogisticRegression(max_iter=1000)
model.fit(X_train, y_train)

# 6. Evaluate
preds = model.predict(X_test)
acc = accuracy_score(y_test, preds)
print(f"Accuracy: {acc:.2f}")

#or

# Step 1: Import Required Libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.preprocessing import StandardScaler

# Step 2: Load and Explore Data
#url = "https://raw.githubusercontent.com/huzaifsayed/Linear-Regression-Model-for-House-Price-Prediction/master/USA_Housing.csv"
data = pd.read_csv('USA_Housing.csv')

print("First 5 rows:")
print(data.head())
print("\nDataset info:")
print(data.info())
print("\nStatistical summary:")
print(data.describe())

# Step 3: Data Visualization
sns.pairplot(data[['Avg. Area Income', 'Avg. Area House Age', 'Avg. Area Number of Rooms',
                   'Avg. Area Number of Bedrooms', 'Area Population', 'Price']])
plt.show()

# Step 4: Data Preprocessing
# Check for missing values
print("\nMissing values:")
print(data.isnull().sum())

# Select features and target
X = data.drop(['Price', 'Address'], axis=1)
y = data['Price']

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Feature scaling
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Step 5: Model Training
model = LinearRegression()
model.fit(X_train_scaled, y_train)

# Step 6: Model Evaluation
# Make predictions
y_pred = model.predict(X_test_scaled)

# Calculate metrics
mae = mean_absolute_error(y_test, y_pred)
mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)
r2 = r2_score(y_test, y_pred)

print("\nModel Performance:")
print(f"MAE: {mae:.2f}")
print(f"MSE: {mse:.2f}")
print(f"RMSE: {rmse:.2f}")
print(f"R² Score: {r2:.4f}")

# Step 7: Visualization of Results
plt.figure(figsize=(12, 6))

# Actual vs Predicted prices
plt.subplot(1, 2, 1)
sns.scatterplot(x=y_test, y=y_pred)
plt.xlabel('Actual Prices')
plt.ylabel('Predicted Prices')
plt.title('Actual vs Predicted Prices')

# Residual plot
plt.subplot(1, 2, 2)
sns.histplot((y_test - y_pred), bins=50, kde=True)
plt.xlabel('Residuals')
plt.title('Residual Distribution')

plt.tight_layout()
plt.show()

# Step 8: Predict New House Prices
def predict_price(input_features):
    """Function to predict house price for new input"""
    scaled_features = scaler.transform([input_features])
    prediction = model.predict(scaled_features)
    return prediction[0]

# Example prediction
new_house = [65000, 5, 7, 4, 25000]  # [Income, House Age, Rooms, Bedrooms, Population]
predicted_price = predict_price(new_house)
print(f"\nPredicted Price for New House: ${predicted_price:,.2f}")

#OR

#House‑Price Category Classification (CNN + Hyperparameter Tuning)
#– Classify houses into price buckets (Low/Medium/High or High vs. Low-value) 
#using a CNN, tuning its architecture and training hyperparameters for best accuracy

import pandas as pd
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout
from tensorflow.keras.utils import to_categorical
from sklearn.model_selection import train_test_split

# 0. Hyperparameters (feel free to tweak)
learning_rate = 0.001
filters       = 32
dropout_rate  = 0.5
epochs        = 10
batch_size    = 32

# 1. Load & bin into 3 categories
#url = "https://raw.githubusercontent.com/huzaifsayed/Linear-Regression-Model-for-House-PricePrediction/master/USA_Housing.csv"
df = pd.read_csv('USA_Housing.csv')
df['Category'] = pd.qcut(df['Price'], 3, labels=False)  # 0=Low,1=Med,2=High

# 2. Prepare X (4 features) & y (one‑hot)
features = ["Avg. Area Income", "Avg. Area House Age", 
            "Avg. Area Number of Rooms", "Area Population"]
X = df[features].values.reshape(-1, len(features), 1)
y = to_categorical(df['Category'], 3)

# 3. Train/test split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=0
)

# 4. Build the 1D‑CNN
model = Sequential([
    Conv1D(filters, kernel_size=2, activation='relu', 
           input_shape=(len(features), 1)),
    MaxPooling1D(),
    Flatten(),
    Dropout(dropout_rate),
    Dense(16, activation='relu'),
    Dense(3, activation='softmax')
])

# 5. Compile & train
opt = tf.keras.optimizers.Adam(learning_rate=learning_rate)
model.compile(optimizer=opt,
              loss='categorical_crossentropy',
              metrics=['accuracy'])
model.fit(X_train, y_train,
          epochs=epochs,
          batch_size=batch_size,
          verbose=2)

# 6. Evaluate
loss, acc = model.evaluate(X_test, y_test, verbose=0)
print(f"Test accuracy: {acc:.2f}")

-----------------------------------------------------------------------------------------------------------------------------------------------------------------------

2. Build a Multiclass classifier using the CNN model. Use MNIST or any other suitable dataset. a. 
Perform Data Pre-processing b. Define Model and perform training c. Evaluate Results using confusion 
matrix.


import numpy as np
from tensorflow.keras.datasets import mnist
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense
from tensorflow.keras.utils import to_categorical
from sklearn.metrics import confusion_matrix

# 1. Load & preprocess
(x_train, y_train), (x_test, y_test) = mnist.load_data()
# reshape to (n,28,28,1) and scale to [0,1]
x_train = x_train.reshape(-1,28,28,1) / 255.0
x_test  = x_test.reshape (-1,28,28,1) / 255.0
# one-hot encode labels
y_train = to_categorical(y_train, 10)
y_test  = to_categorical(y_test, 10)

# 2. Define a tiny CNN
model = Sequential([
    Conv2D(32, (3,3), activation='relu', input_shape=(28,28,1)),
    MaxPooling2D(),
    Flatten(),
    Dense(128, activation='relu'),
    Dense(10, activation='softmax')
])

# 3. Compile & train
model.compile(optimizer='adam',
              loss='categorical_crossentropy',
              metrics=['accuracy'])

model.fit(
    x_train, y_train,
    epochs=3,
    batch_size=128,
    validation_split=0.1,
    verbose=2
)

# 4. Evaluate on test set
loss, acc = model.evaluate(x_test, y_test, verbose=0)
print(f"Test accuracy: {acc:.3f}")

# 5. Confusion matrix
y_pred = model.predict(x_test, verbose=0)
y_pred_classes = np.argmax(y_pred, axis=1)
y_true = np.argmax(y_test, axis=1)

cm = confusion_matrix(y_true, y_pred_classes)
print("Confusion Matrix:")
print(cm)

#or

# Step 1: Import Required Libraries
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.datasets import mnist
from tensorflow.keras.utils import to_categorical
from sklearn.metrics import confusion_matrix, classification_report

# Step 2: Data Loading and Preprocessing
# Load MNIST dataset
(x_train, y_train), (x_test, y_test) = mnist.load_data()

# Normalize pixel values to [0, 1]
x_train = x_train.astype("float32") / 255
x_test = x_test.astype("float32") / 255

# Reshape data to include channel dimension (required for CNN)
x_train = x_train.reshape(60000, 28, 28, 1)
x_test = x_test.reshape(10000, 28, 28, 1)

# Convert class vectors to binary class matrices (one-hot encoding)
y_train = to_categorical(y_train, 10)
y_test = to_categorical(y_test, 10)

# Step 3: Model Definition
model = keras.Sequential([
    # First convolutional layer
    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),
    layers.MaxPooling2D((2, 2)),
    
    # Second convolutional layer
    layers.Conv2D(64, (3, 3), activation='relu'),
    layers.MaxPooling2D((2, 2)),
    
    # Classification layers
    layers.Flatten(),
    layers.Dense(128, activation='relu'),
    layers.Dense(10, activation='softmax')
])

# Compile the model
model.compile(optimizer='adam',
              loss='categorical_crossentropy',
              metrics=['accuracy'])

# Display model architecture
model.summary()

# Step 4: Model Training
history = model.fit(x_train, y_train,
                    batch_size=128,
                    epochs=5,
                    validation_split=0.1)

# Step 5: Model Evaluation
# Evaluate on test set
test_loss, test_acc = model.evaluate(x_test, y_test, verbose=0)
print(f'\nTest accuracy: {test_acc:.4f}')
print(f'Test loss: {test_loss:.4f}')

# Generate predictions
y_pred = model.predict(x_test)
y_pred_classes = np.argmax(y_pred, axis=1)
y_true = np.argmax(y_test, axis=1)

# Generate confusion matrix
cm = confusion_matrix(y_true, y_pred_classes)

# Plot confusion matrix
plt.figure(figsize=(10, 8))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.xlabel('Predicted')
plt.ylabel('True')
plt.show()

# Classification report
print(classification_report(y_true, y_pred_classes))

#or

import tensorflow as tf
from tensorflow.keras.datasets import cifar10
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import (
    Conv2D, MaxPooling2D, Flatten, Dense, Dropout
)
from tensorflow.keras.optimizers import Adam

import matplotlib.pyplot as plt
plt.figure(figsize=(6,6)); [(plt.subplot(2,2,i+1), plt.imshow(x_train[i]), plt.axis('off')) for i in range(4)]; plt.show()


# 1. Hyperparameters you can tweak
learning_rate  = 0.001
num_filters    = 32
dropout_rate   = 0.5
batch_size     = 64
epochs         = 5

# 2. Load & preprocess CIFAR‑10
(x_train, y_train), (x_test, y_test) = cifar10.load_data()
x_train = x_train.astype("float32") / 255.0
x_test  = x_test.astype("float32")  / 255.0
y_train = tf.keras.utils.to_categorical(y_train, 10)
y_test  = tf.keras.utils.to_categorical(y_test, 10)

# 3. Build the model
model = Sequential([
    Conv2D(num_filters, (3,3), activation="relu", padding="same",
           input_shape=x_train.shape[1:]),
    MaxPooling2D(),
    Conv2D(num_filters*2, (3,3), activation="relu", padding="same"),
    MaxPooling2D(),
    Flatten(),
    Dropout(dropout_rate),
    Dense(128, activation="relu"),
    Dropout(dropout_rate),
    Dense(10, activation="softmax")
])

# 4. Compile with chosen optimizer & learning rate
opt = Adam(learning_rate=learning_rate)
model.compile(
    optimizer=opt,
    loss="categorical_crossentropy",
    metrics=["accuracy"]
)

# 5. Train & evaluate
model.fit(
    x_train, y_train,
    batch_size=batch_size,
    epochs=epochs,
    validation_split=0.1,
    verbose=2
)

loss, acc = model.evaluate(x_test, y_test, verbose=0)
print(f"Test accuracy: {acc:.3f}")

#OR

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from tensorflow.keras.datasets import fashion_mnist
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout
from tensorflow.keras.optimizers import Adam
from sklearn.metrics import confusion_matrix

# Load dataset
(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()

# Reshape for CNN input and normalize
X_train = X_train.reshape(-1, 28, 28, 1).astype('float32') / 255.0
X_test = X_test.reshape(-1, 28, 28, 1).astype('float32') / 255.0

# One-hot encode labels
y_train_cat = to_categorical(y_train, 10)
y_test_cat = to_categorical(y_test, 10)

model = Sequential([
    Conv2D(32, (3,3), activation='relu', input_shape=(28,28,1)),
    MaxPooling2D(pool_size=(2,2)),
    Dropout(0.25),

    Conv2D(64, (3,3), activation='relu'),
    MaxPooling2D(pool_size=(2,2)),
    Dropout(0.25),

    Flatten(),
    Dense(128, activation='relu'),
    Dropout(0.5),
    Dense(10, activation='softmax')  # 10 classes
])

optimizer = Adam(learning_rate=0.001)

model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])

# Train model
model.fit(X_train, y_train_cat, epochs=10, batch_size=64, validation_split=0.2)

# Evaluate
test_loss, test_acc = model.evaluate(X_test, y_test_cat)
print(f"Test Accuracy: {test_acc*100:.2f}%")

# Predict classes
y_pred = np.argmax(model.predict(X_test), axis=1)

# Confusion matrix
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(8,6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Purples')
plt.title('Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

#OR

## Transfer Learning for Cats vs Dogs Classification using MobileNetV2
# Beginner-friendly Jupyter Notebook code leveraging a high-performance, predefined CNN
# Dataset - https://www.kaggle.com/datasets/samuelcortinhas/cats-and-dogs-image-classification?select=test

# 1. Imports
import os
import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Dense, Dropout, GlobalAveragePooling2D
from tensorflow.keras.applications import MobileNetV2
from tensorflow.keras.applications.mobilenet_v2 import preprocess_input
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.metrics import confusion_matrix, classification_report

# 2. Hyperparameters
img_height = 160  # MobileNetV2 default
img_width  = 160
batch_size = 16
epochs     = 20
learning_rate = 1e-4
dropout_rate = 0.3
dense_units  = 128

# 3. Paths to dataset (adjust if needed)
train_dir = 'train'   # contains 'cats/' and 'dogs/' subfolders
test_dir  = 'test'    # contains 'cats/' and 'dogs/' subfolders

# 4. Data generators with preprocessing and augmentation
train_datagen = ImageDataGenerator(
    preprocessing_function=preprocess_input,
    rotation_range=30,
    width_shift_range=0.2,
    height_shift_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True,
    validation_split=0.2
)

test_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)

train_generator = train_datagen.flow_from_directory(
    train_dir,
    target_size=(img_height, img_width),
    batch_size=batch_size,
    class_mode='binary',
    subset='training',
    shuffle=True
)

val_generator = train_datagen.flow_from_directory(
    train_dir,
    target_size=(img_height, img_width),
    batch_size=batch_size,
    class_mode='binary',
    subset='validation',
    shuffle=False
)

# 5. Load MobileNetV2 base model (pretrained on ImageNet)
base_model = MobileNetV2(
    weights='imagenet',
    include_top=False,
    input_shape=(img_height, img_width, 3)
)

# Freeze the base model
base_model.trainable = False

# 6. Build the top classifier
x = base_model.output
x = GlobalAveragePooling2D()(x)
x = Dropout(dropout_rate)(x)
outputs = Dense(1, activation='sigmoid')(x)
model = Model(inputs=base_model.input, outputs=outputs)

# 7. Compile the model
optimizer = Adam(learning_rate=learning_rate)
model.compile(
    optimizer=optimizer,
    loss='binary_crossentropy',
    metrics=['accuracy']
)
model.summary()

# 8. Set up EarlyStopping
es = EarlyStopping(
    monitor='val_loss',
    patience=3,
    restore_best_weights=True
)

# 9. Train the model
history = model.fit(
    train_generator,
    epochs=epochs,
    validation_data=val_generator,
    callbacks=[es],
    verbose=1
)

# 10. Plot training history
plt.figure(figsize=(12, 5))

# Accuracy
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='train_acc')
plt.plot(history.history['val_accuracy'], label='val_acc')
plt.title('Accuracy over Epochs')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()

# Loss
plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='train_loss')
plt.plot(history.history['val_loss'], label='val_loss')
plt.title('Loss over Epochs')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

plt.show()


#Optional
# 11. Fine-tune: unfreeze top layers of base model
base_model.trainable = True
for layer in base_model.layers[:-20]:  # freeze all but last 20 layers
    layer.trainable = False

model.compile(
    optimizer=Adam(learning_rate=learning_rate/10),  # lower LR for fine-tuning
    loss='binary_crossentropy',
    metrics=['accuracy']
)

ft_history = model.fit(
    train_generator,
    epochs=10,
    validation_data=val_generator,
    callbacks=[es],
    verbose=1
)

# 12. Evaluate on test set
generator_test = test_datagen.flow_from_directory(
    test_dir,
    target_size=(img_height, img_width),
    batch_size=1,
    class_mode='binary',
    shuffle=False
)

# Predict
y_prob = model.predict(generator_test, verbose=1)
y_pred = (y_prob > 0.5).astype(int).flatten()
y_true = generator_test.classes

# Confusion matrix & report
cm = confusion_matrix(y_true, y_pred)
print("Confusion Matrix:")
print(cm)
print("\nClassification Report:")
print(classification_report(y_true, y_pred, target_names=list(generator_test.class_indices.keys())))

# End of transfer learning practical

-----------------------------------------------------------------------------------------------------------------------------------------------------------------------
3. Design RNN or its variant including LSTM or GRU a) Select a suitable time series dataset.      
Example – predict sentiments based on product reviews b) Apply for prediction 

# Step 1: Import Required Libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from tensorflow import keras
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, classification_report

# Step 2: Load and Explore Data
# Assuming the dataset is named 'movie_reviews.csv'
df = pd.read_csv('2_movie.csv')

# Check data structure
print(df.head())
print("\nLabel distribution:")
print(df['label'].value_counts())

# Step 3: Text Preprocessing
# Convert text to lowercase
df['text'] = df['text'].str.lower()

# Split into features and labels
X = df['text'].values
y = df['label'].values

# Split into train/test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Tokenize text
tokenizer = Tokenizer(num_words=10000, oov_token='<OOV>')
tokenizer.fit_on_texts(X_train)

# Convert text to sequences
train_sequences = tokenizer.texts_to_sequences(X_train)
test_sequences = tokenizer.texts_to_sequences(X_test)

# Pad sequences to uniform length
max_length = 200  # Keep first 200 words of each review
X_train_pad = pad_sequences(train_sequences, maxlen=max_length, truncating='post')
X_test_pad = pad_sequences(test_sequences, maxlen=max_length, truncating='post')

# Step 4: Build LSTM Model
model = Sequential([
    Embedding(input_dim=10000, output_dim=128, input_length=max_length),
    LSTM(64, dropout=0.2, recurrent_dropout=0.2),
    Dense(1, activation='sigmoid')
])

model.compile(loss='binary_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])

# Step 5: Train the Model
history = model.fit(X_train_pad, y_train,
                    epochs=5,
                    batch_size=64,
                    validation_split=0.1)

# Step 6: Evaluate Model
# Plot training history
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.legend()
plt.show()

# Evaluate on test set
test_loss, test_acc = model.evaluate(X_test_pad, y_test)
print(f'\nTest accuracy: {test_acc:.4f}')

# Generate predictions
y_pred = (model.predict(X_test_pad) > 0.5).astype("int32")

# Confusion matrix
print("\nConfusion Matrix:")
print(confusion_matrix(y_test, y_pred))

# Classification report
print("\nClassification Report:")
print(classification_report(y_test, y_pred))

# Step 7: Make Predictions on New Text
def predict_sentiment(text):
    # Preprocess input
    text = text.lower()
    sequence = tokenizer.texts_to_sequences([text])
    padded = pad_sequences(sequence, maxlen=max_length)
    
    # Make prediction
    prediction = model.predict(padded)[0][0]
    sentiment = 'Positive' if prediction > 0.5 else 'Negative'
    confidence = prediction if sentiment == 'Positive' else 1 - prediction
    
    print(f"Review: {text}")
    print(f"Sentiment: {sentiment} (Confidence: {confidence:.2%})")

# Test with sample reviews
sample_reviews = [
    "This movie was absolutely fantastic! The acting was superb and the plot kept me engaged throughout.",
    "A complete waste of time. Terrible acting and nonsensical storyline.",
    "The film had some good moments but overall felt predictable and uninspired."
]

for review in sample_reviews:
    predict_sentiment(review)
    print()

#OR

# Time‑Series Forecasting (RNN/LSTM/GRU)
#– Select a time-series dataset (e.g., historical house prices or sentiment time series) and build an RNN (vanilla, LSTM, or GRU) 
#to predict future values.

import numpy as np
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense
from sklearn.model_selection import train_test_split

# 1. Generate synthetic data (sine wave)
series = np.sin(np.linspace(0, 100, 1000))

# 2. Create sliding windows
def make_windows(s, window_size=10):
    X, y = [], []
    for i in range(len(s) - window_size):
        X.append(s[i:i+window_size])
        y.append(s[i+window_size])
    return np.array(X), np.array(y)

window_size = 10
X, y = make_windows(series, window_size)
X = X.reshape(-1, window_size, 1)   # (samples, timesteps, features)

# 3. Train/test split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=0
)

# 4. Build & compile LSTM model
model = Sequential([
    LSTM(50, input_shape=(window_size, 1)),
    Dense(1)
])
model.compile(optimizer='adam', loss='mse')

# 5. Train
model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=2)

# 6. Evaluate
mse = model.evaluate(X_test, y_test, verbose=0)
print(f"Test MSE: {mse:.3f}")

# 7. Forecast (example)
preds = model.predict(X_test)

#OR

import numpy as np
from tensorflow.keras.datasets import imdb
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense
from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

# Load dataset with only the top 10,000 most common words
vocab_size = 10000
max_len = 200  # pad or truncate reviews to this length

(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=vocab_size)

# Pad sequences
X_train = pad_sequences(X_train, maxlen=max_len)
X_test = pad_sequences(X_test, maxlen=max_len)

model = Sequential([
    Embedding(input_dim=vocab_size, output_dim=64, input_length=max_len),
    LSTM(128),
    Dense(1, activation='sigmoid')  # Binary classification (positive/negative)
])

model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

# Train the model
model.fit(X_train, y_train, epochs=5, batch_size=128, validation_split=0.2)

# Evaluate
loss, accuracy = model.evaluate(X_test, y_test)
print(f"Test Accuracy: {accuracy*100:.2f}%")

# Predict
y_pred = (model.predict(X_test) > 0.5).astype("int32")

# Confusion Matrix
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(6,5))
sns.heatmap(cm, annot=True, fmt='d', cmap='Greens')
plt.title("Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("True")
plt.show()

# Classification Report
print(classification_report(y_test, y_pred, target_names=["Negative", "Positive"]))

#OR

# Sentiment Analysis (RNN)
#– Build an RNN‑based text classifier to predict sentiment (positive/negative) from product or property review text. Use builtin datsets

import numpy as np
from tensorflow.keras.datasets import imdb
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, SimpleRNN, Dense

# 1. Load & preprocess
max_features = 10000  # only use top 10k words
maxlen       = 100    # cut reviews after 100 words
(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)
x_train = pad_sequences(x_train, maxlen=maxlen)
x_test  = pad_sequences(x_test,  maxlen=maxlen)

# 2. Build the RNN model
model = Sequential([
    Embedding(max_features, 32, input_length=maxlen),
    SimpleRNN(32),
    Dense(1, activation='sigmoid')
])

# 3. Compile & train
model.compile(optimizer='adam',
              loss='binary_crossentropy',
              metrics=['accuracy'])
model.fit(
    x_train, y_train,
    epochs=3,
    batch_size=128,
    validation_split=0.1,
    verbose=2
)

# 4. Evaluate
loss, acc = model.evaluate(x_test, y_test, verbose=0)
print(f"Test accuracy: {acc:.2f}")

-----------------------------------------------------------------------------------------------------------------------------------------------------------------------
6. Perform Sentiment Analysis in the network graph using RNN. 

import numpy as np
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

# Sample data
texts = ["I love this product", "This is terrible", "Absolutely great experience"]
labels = [1, 0, 1]  # Binary sentiment

# Tokenization
tokenizer = Tokenizer(num_words=5000)
tokenizer.fit_on_texts(texts)
sequences = tokenizer.texts_to_sequences(texts)
X = pad_sequences(sequences, maxlen=100)

# Convert labels to NumPy array
labels = np.array(labels)

# Model
model = Sequential([
    Embedding(input_dim=5000, output_dim=64),  # removed input_length warning
    LSTM(64),
    Dense(1, activation='sigmoid')
])

model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
model.fit(X, labels, epochs=5)

def predict_sentiment(text):
    seq = tokenizer.texts_to_sequences([text])
    padded = pad_sequences(seq, maxlen=100)
    prediction = model.predict(padded)[0][0]
    return 'positive' if prediction > 0.5 else 'negative'

import networkx as nx

G = nx.DiGraph()

# Add nodes with sentiment
nodes = {
    1: "I love this!",
    2: "This is bad",
    3: "Agreed!"
}
for node_id, text in nodes.items():
    sentiment = predict_sentiment(text)
    G.add_node(node_id, sentiment=sentiment)

# Add edges (e.g., reply or retweet)
G.add_edge(2, 1)  # user 2 replied to user 1
G.add_edge(3, 1)

# Visualize
import matplotlib.pyplot as plt

color_map = []
for node in G:
    color_map.append('green' if G.nodes[node]['sentiment'] == 'positive' else 'red')

nx.draw(G, with_labels=True, node_color=color_map)
plt.show()

-----------------------------------------------------------------------------------------------------------------------------------------------------------------------
1. Import Data from different Sources such as (Excel, Sql Server, Oracle etc.) and load in       
            targeted system. 
2. Data Visualization from Extraction Transformation and Loading (ETL) Process 
3. Perform the Extraction Transformation and Loading (ETL) process to construct the database  
            in the Sql server / Power BI.

import pandas as pd
#from sqlalchemy import create_engine

# Extract from CSV
df_csv = pd.read_csv("USA_Housing.csv")

# Extract from a SQL source (here: SQLite as an example)
#engine = create_engine("sqlite:///data.db")
#df_sql = pd.read_sql("SELECT * FROM housing_table", engine)

# Combine into one DataFrame
#df = pd.concat([df_csv, df_sql], ignore_index=True)
print(df.head())

import pandas as pd
from sqlalchemy import create_engine

# 1) Extract
df = pd.read_csv("USA_Housing.csv")

# 2) Transform: compute price per room
df["price_per_room"] = df["Price"] / df["Avg. Area Number of Rooms"]

# 3) Load into SQLite
#engine = create_engine("sqlite:///housing.db")
#df.to_sql("housing", engine, if_exists="replace", index=False)

import pandas as pd
import matplotlib.pyplot as plt
from sklearn.datasets import load_iris, load_wine

# — Dataset 1: Iris (Extraction stage) —
iris = load_iris()
df1 = pd.DataFrame(iris.data, columns=iris.feature_names)
df1.hist(figsize=(6,4))          # check raw distributions
plt.suptitle("Iris feature distributions")
plt.show()

# — Dataset 2: Wine (Transformation stage) —
wine = load_wine()
df2 = pd.DataFrame(wine.data, columns=wine.feature_names)
# Example transform: alcohol-to-hue ratio
df2["alc_hue"] = df2["alcohol"] / df2["hue"]
df2.boxplot(column="alc_hue", by=load_wine().target, figsize=(6,4))
plt.suptitle("Alcohol/Hue by Wine class")
plt.xlabel("Class")
plt.ylabel("Alcohol ÷ Hue")
plt.show()

#OR

# Import libraries
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# ------------------
# 1. EXTRACTION
# ------------------
# Load raw data from seaborn's built-in dataset
raw_data = sns.load_dataset('titanic')
print("Raw Data Shape:", raw_data.shape)
raw_data.head()

# ------------------
# 2. TRANSFORMATION (Fixed Version)
# ------------------
# Create copy properly to avoid chained assignment warnings
transformed_data = raw_data.copy()

# Handle missing values correctly
# For age (numerical column)
transformed_data['age'] = transformed_data['age'].fillna(
    transformed_data['age'].median()
)

# For deck (categorical column)
# First add 'Unknown' to categories
transformed_data['deck'] = transformed_data['deck'].cat.add_categories('Unknown')
# Then fill NA
transformed_data['deck'] = transformed_data['deck'].fillna('Unknown')

# Create new features
transformed_data['age_group'] = pd.cut(transformed_data['age'],
                                      bins=[0, 18, 30, 50, 100],
                                      labels=['Child', 'Young Adult', 
                                              'Adult', 'Senior'])

# Convert categorical variables
transformed_data['sex'] = transformed_data['sex'].map({'male':0, 'female':1})

# Drop unnecessary columns
transformed_data = transformed_data.drop(['alive', 'alone'], axis=1)

print("\nTransformed Data Shape:", transformed_data.shape)
transformed_data.head()

# ------------------
# 3. LOADING
# ------------------
# Save transformed data to new CSV
transformed_data.to_csv('clean_titanic.csv', index=False)

# ------------------
# 4. VISUALIZATION (Fixed)
# ------------------
plt.figure(figsize=(18, 12))

# 1. Survival Rate by Class
plt.subplot(2, 3, 1)
sns.barplot(x='class', y='survived', data=transformed_data)
plt.title('Survival Rate by Passenger Class')

# 2. Age Distribution
plt.subplot(2, 3, 2)
sns.histplot(transformed_data['age'], bins=30, kde=True)
plt.title('Age Distribution')

# 3. Fare Distribution by Class
plt.subplot(2, 3, 3)
sns.boxplot(x='class', y='fare', data=transformed_data)
plt.title('Fare Distribution by Class')

# 4. Survival Rate by Gender
plt.subplot(2, 3, 4)
sns.countplot(x='sex', hue='survived', data=transformed_data)
plt.title('Survival Count by Gender')
plt.xticks([0,1], ['Male', 'Female'])

# 5. Age Group Survival
plt.subplot(2, 3, 5)
sns.barplot(x='age_group', y='survived', data=transformed_data)
plt.title('Survival Rate by Age Group')

# 6. Correlation Heatmap (Corrected)
plt.subplot(2, 3, 6)
numeric_cols = transformed_data.select_dtypes(include=['int64', 'float64']).columns
sns.heatmap(transformed_data[numeric_cols].corr(), 
           annot=True, 
           cmap='coolwarm',
           fmt=".2f")
plt.title('Feature Correlation Matrix')

plt.tight_layout()
plt.show()

-----------------------------------------------------------------------------------------------------------------------------------------------------------------------

#POWER BI

A. ETL in Power BI Desktop

Launch Power BI Desktop – Download and install from https://powerbi.microsoft.com if you haven’t already.

Get Data (Extraction)

On the Home ribbon, click Get Data > Text/CSV.

Browse to your processed USA_Housing.csv (or Excel) file and click Open.

In the preview dialog, click Transform Data to enter Power Query Editor.

Transform Data (Transformation)

Rename columns for clarity (e.g. “Avg. Area Number of Rooms” → Rooms).

Change data types: ensure Price, Income, etc., are set to Decimal Number, dates (if any) to Date.

Add a calculated column (e.g., Price per Room): – On the Add Column tab, click Custom Column. – Name it PricePerRoom and use the formula:

m Copy Edit = [Price] / [Rooms] Filter or remove any rows/columns you don’t need (e.g., drop the original address field if unused).

When done, click Close & Apply to load the cleaned table into the data model.

(Optional) Connect to Other Sources – To pull in SQL Server or Oracle tables instead of CSV: Get Data > SQL Server (or appropriate connector), enter server/database, and repeat similar transforms in Power Query before applying.

B. Building Reports & Dashboards

Set Up Your Report Canvas – In Report view, you’ll see a blank canvas and the Fields pane listing your loaded table.

Create Key Visuals

Clustered Bar Chart:

Drag Region (or ZIP code) into Axis and Price into Values to compare average prices by region.

Line Chart:

Drag Avg. Area House Age into Axis and Price into Values to see price trend vs. age.

Map Visual:

If you have Latitude/Longitude or Region fields, use the Map visual with Area Population as Size to show density.

Card Visuals:

Add cards for KPIs like Average Price, Average Income, and Average Price Per Room by dragging those measures into Card.

Add Slicers & Filters – Drag PricePerRoom (or your price‐bucket field) into a Slicer to let users filter the report by “Low/Medium/High” segments. – Use the Filters pane to set default filters (e.g., show only houses with Price > $200k).

Format & Arrange – Click each visual and use the Format (paint roller) pane to:

Add titles, data labels, and tooltips

Adjust font sizes and colors to improve readability – Resize and align visuals so the dashboard flows logically.

Create a Dashboard in Power BI Service (Optional)

Save your .pbix file and publish it via Home > Publish.

In Power BI Service (app.powerbi.com), pin visuals from your report to a new dashboard.

Arrange tiles, add text boxes for commentary, and share with stakeholders.

Save & Share – Save your Power BI file. – Export to PDF or PowerPoint, or share the dashboard link for live, interactive exploration.

With these steps you’ll have:

Extracted and cleaned your housing data in Power Query

Loaded it into Power BI’s data model

Built interactive visuals and slicers

Published and shared a polished dashboard for real‑estate insights.

-----------------------------------------------------------------------------------------------------------------------------------------------------------------------

6.Perform the data clustering algorithm using any Clustering algorithm

import pandas as pd
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler

# 1. Load & select features
#url = "https://raw.githubusercontent.com/huzaifsayed/Linear-Regression-Model-for-House-PricePrediction/master/USA_Housing.csv"
df = pd.read_csv('USA_Housing.csv')
features = ["Avg. Area Income", "Avg. Area House Age",
            "Avg. Area Number of Rooms", "Area Population"]
X = df[features]

# 2. Scale features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# 3. Fit K-Means
kmeans = KMeans(n_clusters=3, random_state=0)
labels = kmeans.fit_predict(X_scaled)

# 4. Attach & inspect
df['Cluster'] = labels
print(df['Cluster'].value_counts())
print("\nCluster centers (scaled feature space):\n", kmeans.cluster_centers_)

#OR

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.datasets import load_iris
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.metrics import silhouette_score, adjusted_rand_score

# Load dataset
iris = load_iris()
X = iris.data  # Features
y = iris.target  # Actual labels (for evaluation)

# Standardize features (important for K-Means)
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Initialize model with 3 clusters (we know Iris has 3 species)
kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)

# Fit model and predict clusters
clusters = kmeans.fit_predict(X_scaled)

# Internal evaluation (no labels needed)
print(f"Silhouette Score: {silhouette_score(X_scaled, clusters):.2f}")

# External evaluation (using known labels)
print(f"Adjusted Rand Index: {adjusted_rand_score(y, clusters):.2f}")

# Reduce to 2D for visualization
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled)

# Create subplots
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))

# Plot actual species
ax1.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='viridis')
ax1.set_title('Actual Species')

# Plot clusters
ax2.scatter(X_pca[:, 0], X_pca[:, 1], c=clusters, cmap='viridis')
ax2.set_title('K-Means Clusters')

plt.show()

# Calculate inertia for different K values
inertias = []
for k in range(1, 8):
    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
    kmeans.fit(X_scaled)
    inertias.append(kmeans.inertia_)

# Plot elbow curve
plt.figure(figsize=(8, 5))
plt.plot(range(1, 8), inertias, marker='o')
plt.xlabel('Number of Clusters')
plt.ylabel('Inertia')
plt.title('Elbow Method')
plt.show()

#OR

import pandas as pd
from sklearn.datasets import load_iris
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler

# 1. Load built-in Iris dataset
iris = load_iris()
X = pd.DataFrame(iris.data, columns=iris.feature_names)

# 2. Scale features (good practice for clustering)
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# 3. Apply K-Means clustering
kmeans = KMeans(n_clusters=3, random_state=0)
labels = kmeans.fit_predict(X_scaled)

# 4. Combine data with cluster labels
X['Cluster'] = labels

# 6. Print cluster counts
print("Cluster counts:")
print(X['Cluster'].value_counts())

_______________________________________________________________________________________________________________________________________________________________________

Perform the data classification algorithm using any Classification algorithm

import pandas as pd
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, confusion_matrix

# 1. Load built-in Iris dataset
iris = load_iris()
X = pd.DataFrame(iris.data, columns=iris.feature_names)
y = iris.target

# 2. Split into train & test sets
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=0
)

# 3. Train an SVM classifier
clf = SVC(kernel='rbf', gamma='scale', random_state=0)
clf.fit(X_train, y_train)

# 4. Make predictions & evaluate
y_pred = clf.predict(X_test)
acc = accuracy_score(y_test, y_pred)
print(f"Test Accuracy: {acc:.2f}")

# 5. Confusion matrix as a DataFrame
cm = confusion_matrix(y_test, y_pred)
cm_df = pd.DataFrame(cm, 
                     index=iris.target_names, 
                     columns=iris.target_names)

#OR

# Step 1: Import Libraries
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
import seaborn as sns
import matplotlib.pyplot as plt

# Step 2: Load Dataset
iris = load_iris()
X = iris.data  # Features (sepal length, sepal width, petal length, petal width)
y = iris.target  # Target (0=setosa, 1=versicolor, 2=virginica)

# Step 3: Split Data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Step 4: Create Model
knn = KNeighborsClassifier(n_neighbors=3)  # Simple version with 3 neighbors

# Step 5: Train Model
knn.fit(X_train, y_train)

# Step 6: Make Predictions
y_pred = knn.predict(X_test)

# Step 7: Evaluate Model
print("Accuracy:", accuracy_score(y_test, y_pred))
print("\nConfusion Matrix:")
print(confusion_matrix(y_test, y_pred))
print("\nClassification Report:")
print(classification_report(y_test, y_pred))

# Visualize Confusion Matrix
plt.figure(figsize=(6,4))
sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, cmap='Blues', 
            xticklabels=iris.target_names, yticklabels=iris.target_names)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

# Step 8: Sample Prediction
sample = [[5.1, 3.5, 1.4, 0.2]]  # Sample features
predicted_class = (knn.predict(sample))[0]
print(f"\nSample Prediction: {iris.target_names[predicted_class]}")

_______________________________________________________________________________________________________________________________________________________________________

Data Analysis and Visualization using Advanced Excel

Advanced Excel Analysis & Dashboarding
Below is a simple, step‑by‑step guide to load your housing dataset into Excel, analyze it with PivotTables and advanced formulas, then assemble an interactive dashboard.

1. Prepare Your Data
Open Excel and go to Data → Get Data → From Text/CSV.

Select the USA_Housing.csv file and click Load (or Transform Data if you need to clean).

Once loaded, click Home → Format as Table and choose a style. Ensure My table has headers is checked.

Rename this table to HousingData (in Table Design → Table Name).

2. Add Calculated Columns
On the HousingData sheet, to enrich your data:

Price per Room

In a new column header cell (e.g., cell G1), type PricePerRoom.

In G2 enter:

excel
Copy
Edit
=[@Price] / [@[Avg. Area Number of Rooms]]
Press Enter—Excel will auto‑fill the formula down the table.

Price Bucket (Low/Medium/High)

In H1, type PriceBucket.

In H2 enter:

excel
Copy
Edit
=IF([@Price] < PERCENTILE.INC(HousingData[Price],0.33),"Low",
  IF([@Price] < PERCENTILE.INC(HousingData[Price],0.67),"Medium","High"))
Press Enter to auto‑fill.

3. Create PivotTables for Analysis
Go to Insert → PivotTable, choose Use this workbook’s Data Model, and click OK.

In the PivotTable Fields pane:

Drag PriceBucket into Filters.

Drag Region (or ZIP code) into Rows.

Drag Price, PricePerRoom, and Area Population into Values—set each to Average (right‑click → Value Field Settings).

Rename your PivotTable sheet to “Summary”.

4. Build Charts from PivotTables
Select the PivotTable on Summary, then go to PivotTable Analyze → PivotChart.

Choose a Clustered Column for average prices by region.

Repeat for a Line Chart showing Average PricePerRoom over PriceBucket (filter the PivotTable accordingly).

Position charts neatly beneath or beside your PivotTable.

5. Add Slicers for Interactivity
Click your PivotTable, then PivotTable Analyze → Insert Slicer.

Check PriceBucket and Avg. Area House Age (or any other field you’d like to filter by).

Move and size your slicers on the Summary sheet. Now clicking a bucket or age category instantly updates both the table and charts.

6. Use Advanced Formulas & Conditional Formatting
Top 10% Highlight

Select the Average of Price column in your PivotTable.

Home → Conditional Formatting → Top/Bottom Rules → Top 10%. Choose a fill color.

Dynamic KPI Cards

On a new sheet named Dashboard, link key metrics by typing = then clicking the PivotTable cell. For example:

excel
Copy
Edit
=GETPIVOTDATA("Price",Summary!$A$3,"PriceBucket","High")
Format each cell as a Card (Home → Cell Styles or draw a Shape and link its text to your cell).

7. Assemble the Dashboard
On the Dashboard sheet, paste your KPI cards (Average Price, Avg PricePerRoom, Avg Population).

Copy your PivotCharts and Slicers from Summary into Dashboard.

Arrange everything: place cards at the top, charts below, slicers to the side.

Add titles and text boxes (Insert → Text Box) to label sections.

8. Final Touches & Sharing
Protect Layout: Review → Protect Sheet (so users can’t accidentally move charts).

Review Interactivity: Click slicers, verify charts and KPIs update instantly.

Share: Save as .xlsx or export to PDF via File → Export → Create PDF/XPS.

You now have an Excel dashboard that lets viewers slice by price buckets or house age, highlights top segments, and shows key metrics and trend charts—all built with PivotTables, charts, slicers, and a handful of advanced formulas.
