


























































































































1. Problem Statement – Real estate agents want help to predict the house price for regions in the USA. 
He gave you the dataset to work on and you decided to use the Linear Regression Model. Create a 
model that will help him to estimate what the house would sell for. 
URL for a dataset: 
https://github.com/huzaifsayed/Linear-Regression-Model-for-House-Price
Prediction/blob/master/USA_Housing.csv 


import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score

# Load dataset
df = pd.read_csv('USA_Housing.csv')

# Explore dataset
print(df.head())
print(df.info())
print(df.describe())

# Check for missing values
print(df.isnull().sum())

# Drop rows with missing target or fill missing values if needed
df = df.dropna()

# Select features and target
# You may need to change these based on actual column names
X = df.drop('Price', axis=1)   # assuming 'price' is the target column
y = df['Price']

# Encode categorical variables if any
X = pd.get_dummies(X, drop_first=True)

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train model
model = LinearRegression()
model.fit(X_train, y_train)

# Predict
y_pred = model.predict(X_test)

# Evaluate
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"Mean Squared Error: {mse}")
print(f"R^2 Score: {r2:.2f}")

# Plot actual vs predicted
plt.scatter(y_test, y_pred)
plt.xlabel("Actual Prices")
plt.ylabel("Predicted Prices")
plt.title("Actual vs Predicted House Prices")
plt.show()



#################### Deep Learning ##############################################

2. Build a Multiclass classifier using the CNN model. Use MNIST or any other suitable dataset. a. 
Perform Data Pre-processing b. Define Model and perform training c. Evaluate Results using confusion 
matrix. 

import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras.datasets import mnist
from tensorflow.keras.utils import to_categorical

# Load dataset
(X_train, y_train), (X_test, y_test) = mnist.load_data()

# Normalize the pixel values
X_train = X_train.astype("float32") / 255.0
X_test = X_test.astype("float32") / 255.0

# Reshape to add channel dimension (28,28,1)
X_train = X_train.reshape((-1, 28, 28, 1))
X_test = X_test.reshape((-1, 28, 28, 1))

# One-hot encode labels
y_train_cat = to_categorical(y_train)
y_test_cat = to_categorical(y_test)

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense

# Define CNN model
model = Sequential([
    Conv2D(32, kernel_size=(3,3), activation='relu', input_shape=(28,28,1)),
    MaxPooling2D(pool_size=(2,2)),
    Conv2D(64, kernel_size=(3,3), activation='relu'),
    MaxPooling2D(pool_size=(2,2)),
    Flatten(),
    Dense(128, activation='relu'),
    Dense(10, activation='softmax')  # 10 classes for digits 0–9
])

# Compile the model
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Train the model
model.fit(X_train, y_train_cat, epochs=5, batch_size=64, validation_split=0.1)


from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

# Predict class labels
y_pred_probs = model.predict(X_test)
y_pred = np.argmax(y_pred_probs, axis=1)

# Confusion matrix
cm = confusion_matrix(y_test, y_pred)

# Display the confusion matrix
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=range(10))
disp.plot(cmap=plt.cm.Blues)
plt.title("Confusion Matrix for MNIST CNN Classifier")
plt.show()


###########################################################################

3. Design RNN or its variant including LSTM or GRU a) Select a suitable time series dataset.      
Example – predict sentiments based on product reviews b) Apply for prediction 


import numpy as np
from tensorflow.keras.datasets import imdb
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense
from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

# Load dataset with only the top 10,000 most common words
vocab_size = 10000
max_len = 200  # pad or truncate reviews to this length

(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=vocab_size)

# Pad sequences
X_train = pad_sequences(X_train, maxlen=max_len)
X_test = pad_sequences(X_test, maxlen=max_len)

model = Sequential([
    Embedding(input_dim=vocab_size, output_dim=64, input_length=max_len),
    LSTM(128),
    Dense(1, activation='sigmoid')  # Binary classification (positive/negative)
])

model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

# Train the model
model.fit(X_train, y_train, epochs=5, batch_size=128, validation_split=0.2)


# Evaluate
loss, accuracy = model.evaluate(X_test, y_test)
print(f"Test Accuracy: {accuracy*100:.2f}%")

# Predict
y_pred = (model.predict(X_test) > 0.5).astype("int32")

# Confusion Matrix
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(6,5))
sns.heatmap(cm, annot=True, fmt='d', cmap='Greens')
plt.title("Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("True")
plt.show()

# Classification Report
print(classification_report(y_test, y_pred, target_names=["Negative", "Positive"]))



##################################################################################

4. Design and implement a CNN for Image Classification a) Select a suitable image classification
dataset (medical imaging, agricultural, etc.). b) Optimized with different hyper-parameters including
learning rate, filter size, no. of layers, optimizers, dropouts, etc.


import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from tensorflow.keras.datasets import fashion_mnist
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout
from tensorflow.keras.optimizers import Adam
from sklearn.metrics import confusion_matrix

# Load dataset
(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()

# Reshape for CNN input and normalize
X_train = X_train.reshape(-1, 28, 28, 1).astype('float32') / 255.0
X_test = X_test.reshape(-1, 28, 28, 1).astype('float32') / 255.0

# One-hot encode labels
y_train_cat = to_categorical(y_train, 10)
y_test_cat = to_categorical(y_test, 10)

model = Sequential([
    Conv2D(32, (3,3), activation='relu', input_shape=(28,28,1)),
    MaxPooling2D(pool_size=(2,2)),
    Dropout(0.25),

    Conv2D(64, (3,3), activation='relu'),
    MaxPooling2D(pool_size=(2,2)),
    Dropout(0.25),

    Flatten(),
    Dense(128, activation='relu'),
    Dropout(0.5),
    Dense(10, activation='softmax')  # 10 classes
])

optimizer = Adam(learning_rate=0.001)

model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])

# Train model
model.fit(X_train, y_train_cat, epochs=10, batch_size=64, validation_split=0.2)

# Evaluate
test_loss, test_acc = model.evaluate(X_test, y_test_cat)
print(f"Test Accuracy: {test_acc*100:.2f}%")

# Predict classes
y_pred = np.argmax(model.predict(X_test), axis=1)

# Confusion matrix
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(8,6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Purples')
plt.title('Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()


###############################################################################################


5.Perform Sentiment Analysis in the network graph using RNN.


import numpy as np
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

# Sample data
texts = ["I love this product", "This is terrible", "Absolutely great experience"]
labels = [1, 0, 1]  # Binary sentiment

# Tokenization
tokenizer = Tokenizer(num_words=5000)
tokenizer.fit_on_texts(texts)
sequences = tokenizer.texts_to_sequences(texts)
X = pad_sequences(sequences, maxlen=100)

# Convert labels to NumPy array
labels = np.array(labels)

# Model
model = Sequential([
    Embedding(input_dim=5000, output_dim=64),  # removed input_length warning
    LSTM(64),
    Dense(1, activation='sigmoid')
])

model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
model.fit(X, labels, epochs=5)

def predict_sentiment(text):
    seq = tokenizer.texts_to_sequences([text])
    padded = pad_sequences(seq, maxlen=100)
    prediction = model.predict(padded)[0][0]
    return 'positive' if prediction > 0.5 else 'negative'

import networkx as nx

G = nx.DiGraph()

# Add nodes with sentiment
nodes = {
    1: "I love this!",
    2: "This is bad",
    3: "Agreed!"
}
for node_id, text in nodes.items():
    sentiment = predict_sentiment(text)
    G.add_node(node_id, sentiment=sentiment)

# Add edges (e.g., reply or retweet)
G.add_edge(2, 1)  # user 2 replied to user 1
G.add_edge(3, 1)

# Visualize
import matplotlib.pyplot as plt

color_map = []
for node in G:
    color_map.append('green' if G.nodes[node]['sentiment'] == 'positive' else 'red')

nx.draw(G, with_labels=True, node_color=color_map)
plt.show()

########################################################################################################

6. Data Visualization from Extraction Transformation and Loading (ETL) Process


# Import libraries
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# ------------------
# 1. EXTRACTION
# ------------------
# Load raw data from seaborn's built-in dataset
raw_data = sns.load_dataset('titanic')
print("Raw Data Shape:", raw_data.shape)
raw_data.head()

# ------------------
# 2. TRANSFORMATION (Fixed Version)
# ------------------
# Create copy properly to avoid chained assignment warnings
transformed_data = raw_data.copy()

# Handle missing values correctly
# For age (numerical column)
transformed_data['age'] = transformed_data['age'].fillna(
    transformed_data['age'].median()
)

# For deck (categorical column)
# First add 'Unknown' to categories
transformed_data['deck'] = transformed_data['deck'].cat.add_categories('Unknown')
# Then fill NA
transformed_data['deck'] = transformed_data['deck'].fillna('Unknown')

# Create new features
transformed_data['age_group'] = pd.cut(transformed_data['age'],
                                      bins=[0, 18, 30, 50, 100],
                                      labels=['Child', 'Young Adult', 
                                              'Adult', 'Senior'])

# Convert categorical variables
transformed_data['sex'] = transformed_data['sex'].map({'male':0, 'female':1})

# Drop unnecessary columns
transformed_data = transformed_data.drop(['alive', 'alone'], axis=1)

print("\nTransformed Data Shape:", transformed_data.shape)
transformed_data.head()

# ------------------
# 3. LOADING
# ------------------
# Save transformed data to new CSV
transformed_data.to_csv('clean_titanic.csv', index=False)

# ------------------
# 4. VISUALIZATION (Fixed)
# ------------------
plt.figure(figsize=(18, 12))

# 1. Survival Rate by Class
plt.subplot(2, 3, 1)
sns.barplot(x='class', y='survived', data=transformed_data)
plt.title('Survival Rate by Passenger Class')

# 2. Age Distribution
plt.subplot(2, 3, 2)
sns.histplot(transformed_data['age'], bins=30, kde=True)
plt.title('Age Distribution')

# 3. Fare Distribution by Class
plt.subplot(2, 3, 3)
sns.boxplot(x='class', y='fare', data=transformed_data)
plt.title('Fare Distribution by Class')

# 4. Survival Rate by Gender
plt.subplot(2, 3, 4)
sns.countplot(x='sex', hue='survived', data=transformed_data)
plt.title('Survival Count by Gender')
plt.xticks([0,1], ['Male', 'Female'])

# 5. Age Group Survival
plt.subplot(2, 3, 5)
sns.barplot(x='age_group', y='survived', data=transformed_data)
plt.title('Survival Rate by Age Group')

# 6. Correlation Heatmap (Corrected)
plt.subplot(2, 3, 6)
numeric_cols = transformed_data.select_dtypes(include=['int64', 'float64']).columns
sns.heatmap(transformed_data[numeric_cols].corr(), 
           annot=True, 
           cmap='coolwarm',
           fmt=".2f")
plt.title('Feature Correlation Matrix')

plt.tight_layout()
plt.show()


###########################################################################################

9. Perform the data classification algorithm using any Classification algorithm 


# Step 1: Import Libraries
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
import seaborn as sns
import matplotlib.pyplot as plt

# Step 2: Load Dataset
iris = load_iris()
X = iris.data  # Features (sepal length, sepal width, petal length, petal width)
y = iris.target  # Target (0=setosa, 1=versicolor, 2=virginica)

# Step 3: Split Data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Step 4: Create Model
knn = KNeighborsClassifier(n_neighbors=3)  # Simple version with 3 neighbors

# Step 5: Train Model
knn.fit(X_train, y_train)

# Step 6: Make Predictions
y_pred = knn.predict(X_test)

# Step 7: Evaluate Model
print("Accuracy:", accuracy_score(y_test, y_pred))
print("\nConfusion Matrix:")
print(confusion_matrix(y_test, y_pred))
print("\nClassification Report:")
print(classification_report(y_test, y_pred))

# Visualize Confusion Matrix
plt.figure(figsize=(6,4))
sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, cmap='Blues', 
            xticklabels=iris.target_names, yticklabels=iris.target_names)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

# Step 8: Sample Prediction
sample = [[5.1, 3.5, 1.4, 0.2]]  # Sample features
predicted_class = (knn.predict(sample))[0]
print(f"\nSample Prediction: {iris.target_names[predicted_class]}")


#######################################################################################

10. Perform the data clustering algorithm using any Clustering algorithm

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.datasets import load_iris
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.metrics import silhouette_score, adjusted_rand_score

# Load dataset
iris = load_iris()
X = iris.data  # Features
y = iris.target  # Actual labels (for evaluation)

# Standardize features (important for K-Means)
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Initialize model with 3 clusters (we know Iris has 3 species)
kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)

# Fit model and predict clusters
clusters = kmeans.fit_predict(X_scaled)

# Internal evaluation (no labels needed)
print(f"Silhouette Score: {silhouette_score(X_scaled, clusters):.2f}")

# External evaluation (using known labels)
print(f"Adjusted Rand Index: {adjusted_rand_score(y, clusters):.2f}")

# Reduce to 2D for visualization
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled)

# Create subplots
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))

# Plot actual species
ax1.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='viridis')
ax1.set_title('Actual Species')

# Plot clusters
ax2.scatter(X_pca[:, 0], X_pca[:, 1], c=clusters, cmap='viridis')
ax2.set_title('K-Means Clusters')

plt.show()

# Calculate inertia for different K values
inertias = []
for k in range(1, 8):
    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
    kmeans.fit(X_scaled)
    inertias.append(kmeans.inertia_)

# Plot elbow curve
plt.figure(figsize=(8, 5))
plt.plot(range(1, 8), inertias, marker='o')
plt.xlabel('Number of Clusters')
plt.ylabel('Inertia')
plt.title('Elbow Method')
plt.show()


//// Theory: Key Theory Explained K-Means Algorithm

Step 1: Randomly initialize K cluster centroids
Step 2: Assign points to nearest centroid
Step 3: Recalculate centroids as cluster means
Step 4: Repeat until convergence (no changes)
Key Concepts
Inertia: Sum of squared distances to nearest centroid (lower = better)
Silhouette Score: Measures cluster cohesion/separation (-1 to 1)
Adjusted Rand Index: Compares cluster similarity to true labels (0-1)
Standardization
Essential for distance-based algorithms
Formula:
math z = \frac{x - \mu}{\sigma} Elbow Method
Helps choose optimal K
Look for "elbow" point where inertia decrease slows
Interpretation Silhouette Score (0.50): Moderate cluster separation
Adjusted Rand Index (0.73): Good alignment with actual species
Elbow Plot: Confirms K=3 is optimal for this dataset
Real-World Applications Customer segmentation
Image compression (color quantization)
Document clustering
Anomaly detection


##################################################################################################################
##################################################################################################################

13. To better target their marketing strategies, a real estate agency wants to classify houses 
into price categories: Low, Medium, and High. Build a Convolutional Neural Network 
(CNN) to classify houses based on the features provided in the dataset

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout
from tensorflow.keras.utils import to_categorical
import numpy as np

# Load dataset
(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()

# Simulate 3 classes (e.g., automobile=1, truck=9, ship=8)
selected_classes = [1, 8, 9]  # Simulating Low, Medium, High
def filter_classes(x, y):
    idx = np.isin(y, selected_classes).flatten()
    x, y = x[idx], y[idx]
    # Map to 0, 1, 2
    y = np.array([selected_classes.index(label) for label in y.flatten()])
    return x, y

x_train, y_train = filter_classes(x_train, y_train)
x_test, y_test = filter_classes(x_test, y_test)

# Normalize and one-hot encode
x_train, x_test = x_train / 255.0, x_test / 255.0
y_train, y_test = to_categorical(y_train, 3), to_categorical(y_test, 3)


model = Sequential([
    Conv2D(32, (3,3), activation='relu', input_shape=(32,32,3)),
    MaxPooling2D(2,2),

    Conv2D(64, (3,3), activation='relu'),
    MaxPooling2D(2,2),

    Flatten(),
    Dense(128, activation='relu'),
    Dropout(0.5),
    Dense(3, activation='softmax')
])


model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

model.fit(x_train, y_train, epochs=10, validation_data=(x_test, y_test))


loss, acc = model.evaluate(x_test, y_test)
print(f"Test Accuracy: {acc:.2f}")


##########################################################################################################

14. A financial analyst wants to model house price trends over increasing area numbers of 
rooms. Use an LSTM-based Recurrent Neural Network to predict the next house price 
value based on historical patterns in sorted data. 

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense


# Generate synthetic data
np.random.seed(0)
area = np.arange(500, 3000, 50)
rooms = np.round(area / 500 + np.random.randn(len(area))*0.5).astype(int)
price = area * 300 + rooms * 10000 + np.random.randn(len(area)) * 10000

df = pd.DataFrame({'Area': area, 'Rooms': rooms, 'Price': price})
df = df.sort_values(by=['Area', 'Rooms']).reset_index(drop=True)


scaler = MinMaxScaler()
scaled_data = scaler.fit_transform(df[['Area', 'Rooms', 'Price']])

sequence_length = 5
X, y = [], []

for i in range(len(scaled_data) - sequence_length):
    X.append(scaled_data[i:i+sequence_length, 0:2])  # Area and Rooms
    y.append(scaled_data[i+sequence_length, 2])      # Price

X = np.array(X)
y = np.array(y)


model = Sequential([
    LSTM(64, input_shape=(X.shape[1], X.shape[2]), return_sequences=False),
    Dense(32, activation='relu'),
    Dense(1)
])

model.compile(optimizer='adam', loss='mse')
model.summary()


model.fit(X, y, epochs=20, batch_size=8, verbose=1)


predicted = model.predict(X)
actual_price = scaler.inverse_transform(np.hstack([X[:, -1], y.reshape(-1,1)]))[:, -1]
predicted_price = scaler.inverse_transform(np.hstack([X[:, -1], predicted]))[:, -1]

plt.plot(actual_price, label='Actual Price')
plt.plot(predicted_price, label='Predicted Price')
plt.legend()
plt.title("Actual vs Predicted House Prices")
plt.show()

#####################################################################################################

15.  To streamline the approval of housing loans, a bank wants to classify houses into either 
"High-value" or "Low-value" categories. Implement a CNN model and optimize 
hyperparameters such as the number of layers, filter size, dropout, and learning rate to 
improve classification accuracy


pip install keras-tuner

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.utils import to_categorical
from kerastuner.tuners import RandomSearch
import numpy as np


# Load CIFAR-10
(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()

# Select 2 classes to simulate High-value (class 0) and Low-value (class 1)
classes = [0, 1]  # airplane and automobile as stand-ins
train_idx = np.isin(y_train, classes).flatten()
test_idx = np.isin(y_test, classes).flatten()

x_train, y_train = x_train[train_idx], y_train[train_idx]
x_test, y_test = x_test[test_idx], y_test[test_idx]

# Re-label to 0 and 1
y_train = np.array([classes.index(i) for i in y_train.flatten()])
y_test = np.array([classes.index(i) for i in y_test.flatten()])

# Normalize and one-hot encode
x_train, x_test = x_train / 255.0, x_test / 255.0
y_train, y_test = to_categorical(y_train, 2), to_categorical(y_test, 2)


def build_model(hp):
    model = Sequential()
    model.add(Conv2D(
        filters=hp.Choice('filters_1', [32, 64, 128]),
        kernel_size=hp.Choice('kernel_size_1', [3, 5]),
        activation='relu',
        input_shape=(32, 32, 3)
    ))
    model.add(MaxPooling2D(pool_size=2))

    model.add(Conv2D(
        filters=hp.Choice('filters_2', [64, 128]),
        kernel_size=hp.Choice('kernel_size_2', [3, 5]),
        activation='relu'
    ))
    model.add(MaxPooling2D(pool_size=2))
    model.add(Flatten())

    model.add(Dense(
        units=hp.Choice('dense_units', [64, 128, 256]),
        activation='relu'
    ))
    model.add(Dropout(hp.Choice('dropout', [0.3, 0.5])))

    model.add(Dense(2, activation='softmax'))

    model.compile(
        optimizer=Adam(hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4])),
        loss='categorical_crossentropy',
        metrics=['accuracy']
    )
    return model


tuner = RandomSearch(
    build_model,
    objective='val_accuracy',
    max_trials=5,
    directory='house_value_tuning',
    project_name='house_value_cnn'
)

tuner.search(x_train, y_train, epochs=5, validation_split=0.2)


best_model = tuner.get_best_models(num_models=1)[0]
history = best_model.fit(x_train, y_train, epochs=10, validation_data=(x_test, y_test))


loss, accuracy = best_model.evaluate(x_test, y_test)
print(f"Test Accuracy: {accuracy:.2f}")


################################################################################################################

16.   A property review site wants to predict user sentiment (positive or negative) based on 
their written reviews. Build a sentiment classification model using an RNN on simulated 
review text data. 


import numpy as np
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

reviews = [
    "Great house, very spacious and clean",
    "Terrible experience, the place was dirty",
    "Loved the location and the view",
    "Worst property ever, not worth the price",
    "Amazing experience, will visit again",
    "Very noisy and uncomfortable",
    "Perfect for a family vacation",
    "Bad neighborhood and rude staff",
    "Highly recommended, beautiful stay",
    "Disappointing, not as described"
]

labels = [1, 0, 1, 0, 1, 0, 1, 0, 1, 0]  # 1 = positive, 0 = negative


tokenizer = Tokenizer(num_words=1000, oov_token="<OOV>")
tokenizer.fit_on_texts(reviews)

sequences = tokenizer.texts_to_sequences(reviews)
padded_sequences = pad_sequences(sequences, padding='post', maxlen=10)


model = Sequential([
    Embedding(input_dim=1000, output_dim=16, input_length=10),
    LSTM(64),
    Dropout(0.5),
    Dense(1, activation='sigmoid')
])

model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
model.summary()


model.fit(padded_sequences, np.array(labels), epochs=10, verbose=1)


test_review = ["The room was dirty and smelled bad"]
test_seq = tokenizer.texts_to_sequences(test_review)
test_pad = pad_sequences(test_seq, maxlen=10, padding='post')
prediction = model.predict(test_pad)

print("Sentiment:", "Positive" if prediction[0][0] > 0.5 else "Negative")


###############################################################################################

17.18.19 nhi ahe

#################################################################################################
20.  To prioritize high-value listings, a real estate firm wants to predict whether a house falls 
in the "High" or "Low" price category. Build a Random Forest classifier to perform binary 
classification based on the house’s features. 



import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, accuracy_score


# Simulate dataset with features: Area (sqft), Rooms, Age (years), Distance to city center (km), etc.
np.random.seed(42)
data_size = 200

data = pd.DataFrame({
    'area': np.random.randint(500, 3000, size=data_size),
    'rooms': np.random.randint(1, 6, size=data_size),
    'age': np.random.randint(0, 30, size=data_size),
    'distance_to_city': np.random.uniform(1, 20, size=data_size),
})

# Assign labels: 1 = High, 0 = Low (based on simple rule)
data['price_category'] = ((data['area'] > 2000) & (data['rooms'] >= 4)).astype(int)


X = data.drop('price_category', axis=1)
y = data['price_category']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)


model = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=42)
model.fit(X_train, y_train)


y_pred = model.predict(X_test)

print("Accuracy:", accuracy_score(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred))


import matplotlib.pyplot as plt

importances = model.feature_importances_
features = X.columns

plt.barh(features, importances)
plt.xlabel("Feature Importance")
plt.title("Random Forest - Feature Importances")
plt.show()

###############################################################################################

21 .  To identify regional housing market segments, an analyst wants to group properties 
based on similar features. Apply the K-Means clustering algorithm to segment the 
housing dataset and visualize the clusters.


import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler


# Simulated dataset: features = area, number of rooms, house age, and distance to city
np.random.seed(42)
data = pd.DataFrame({
    'area': np.random.randint(500, 4000, 200),
    'rooms': np.random.randint(1, 6, 200),
    'age': np.random.randint(0, 30, 200),
    'distance_to_city': np.random.uniform(1, 25, 200)
})


scaler = StandardScaler()
scaled_data = scaler.fit_transform(data)


# Apply KMeans with 3 clusters
kmeans = KMeans(n_clusters=3, random_state=42)
clusters = kmeans.fit_predict(scaled_data)

# Add cluster labels to original data
data['cluster'] = clusters


# 2D plot using area vs distance_to_city
plt.figure(figsize=(8, 6))
sns.scatterplot(x='area', y='distance_to_city', hue='cluster', data=data, palette='Set2')
plt.title('K-Means Clustering of Properties')
plt.xlabel('Area (sqft)')
plt.ylabel('Distance to City (km)')
plt.show()


wcss = []
for i in range(1, 10):
    kmeans = KMeans(n_clusters=i, random_state=42)
    kmeans.fit(scaled_data)
    wcss.append(kmeans.inertia_)

plt.plot(range(1, 10), wcss, marker='o')
plt.title("Elbow Method for Optimal k")
plt.xlabel("Number of Clusters")
plt.ylabel("WCSS")
plt.show()


#######################################   THE END   ##########################################################

